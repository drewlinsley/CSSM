{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdLGK4UcO8Xn"
   },
   "source": [
    "# TransformerCSSM: Iterative Attention for Visual Reasoning\n",
    "\n",
    "This notebook introduces **TransformerCSSM**, a model that combines the parallelizability of state space models with the expressiveness of transformer-style attention.\n",
    "\n",
    "**Model Performance:** 88.50% accuracy on Pathfinder-14\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **What is CSSM?** - Convolutions in spectral domain, log-space stability\n",
    "2. **The Transformer's Trick** - Instant attention at all positions\n",
    "3. **When Iterative Attention Helps** - The Pathfinder task\n",
    "4. **CSSM as a Solution** - Parallel RNNs, but limited expressiveness\n",
    "5. **Insights from the hGRU** - Bilinear terms and growing receptive fields\n",
    "6. **TransformerCSSM** - Bringing it all together\n",
    "7. **Hands-On Analysis** - Gradients, mechanisms, and interpretability"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Setup: Install dependencies and download checkpoint { display-mode: \"form\" }\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install JAX with GPU support\n",
    "    !pip install -q jax[cuda12] flax optax tensorflow\n",
    "\n",
    "    # Clone CSSM repo\n",
    "    if not os.path.exists('CSSM'):\n",
    "        !git clone https://github.com/your-repo/CSSM.git\n",
    "        sys.path.insert(0, 'CSSM')\n",
    "\n",
    "    # Download checkpoint (--no-check-certificate for expired SSL cert)\n",
    "    CHECKPOINT_URL = \"https://connectomics.clps.brown.edu/tf_records/transformer_cssm_kqv64_epoch20.pkl\"\n",
    "    CHECKPOINT_PATH = \"transformer_cssm_checkpoint.pkl\"\n",
    "\n",
    "    if not os.path.exists(CHECKPOINT_PATH):\n",
    "        print(f\"Downloading checkpoint from {CHECKPOINT_URL}...\")\n",
    "        !wget -q --no-check-certificate {CHECKPOINT_URL} -O {CHECKPOINT_PATH}\n",
    "        print(\"Download complete!\")\n",
    "else:\n",
    "    # Local paths\n",
    "    CHECKPOINT_PATH = \"checkpoints/KQV_64/epoch_20/checkpoint.pkl\"\n",
    "\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")\n",
    "print(f\"Running in {'Colab' if IN_COLAB else 'local'} environment\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmWsKC66O8Xn",
    "outputId": "e6720df7-85a1-477d-deb9-d691343f123a"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.2/581.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.6/648.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.9/200.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m338.1/338.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.5/366.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.7/152.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCloning into 'CSSM'...\n",
      "fatal: could not read Username for 'https://github.com': No such device or address\n",
      "Downloading checkpoint from https://connectomics.clps.brown.edu/tf_records/transformer_cssm_kqv64_epoch20.pkl...\n",
      "Download complete!\n",
      "Checkpoint path: transformer_cssm_checkpoint.pkl\n",
      "Running in Colab environment\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZPS1rfEO8Xo"
   },
   "source": [
    "---\n",
    "# Part I: Background\n",
    "---\n",
    "\n",
    "## 1. What is CSSM?\n",
    "\n",
    "**CSSM (Cepstral State Space Model)** is a recurrent neural network designed for efficient visual processing. Think of it as an RNN that can run in parallel.\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "A standard RNN updates its hidden state like this:\n",
    "\n",
    "$$h_{t+1} = A \\cdot h_t + B \\cdot x_t$$\n",
    "\n",
    "The problem? Each timestep depends on the previous one, so you must compute them **sequentially**. For T timesteps, that's O(T) serial operations—slow on GPUs that thrive on parallelism.\n",
    "\n",
    "### Why Spectral Domain?\n",
    "\n",
    "CSSM performs spatial convolutions using the **Fast Fourier Transform (FFT)**:\n",
    "\n",
    "$$\\text{Conv}(K, X) = F^{-1}(F(K) \\odot F(X))$$\n",
    "\n",
    "**In plain English:** Instead of sliding a kernel across an image (expensive), we:\n",
    "1. Transform both kernel and image to frequency domain (FFT)\n",
    "2. Multiply them element-wise (cheap!)\n",
    "3. Transform back (inverse FFT)\n",
    "\n",
    "This reduces spatial convolution from O(N²) to O(N log N). This change is critical for a recurrent network, as the kernel size (N) will grow over time. In the spectral domain, the kernel size is fixed regardless of the number of timesteps!\n",
    "\n",
    "### Why Log-Space (GOOM)?\n",
    "\n",
    "Another fundamental problem with RNNs is training. When you multiply many numbers together over time (like decay rates), values can explode or vanish:\n",
    "\n",
    "$$h_T = \\lambda^T \\cdot h_0$$\n",
    "\n",
    "**GOOM (Generalized Order of Magnitude)** solves this by working in log-space, where multiplications become additions. But standard log-space can't handle negative numbers—a problem since state matrices have negative eigenvalues.\n",
    "\n",
    "GOOM's trick: encode sign in a complex angle:\n",
    "\n",
    "$$\\text{goom.log}(x) = \\log |x| + i \\theta_x \\quad \\text{where } \\theta_x = \\begin{cases} 0 & x > 0 \\\\ \\pi & x < 0 \\end{cases}$$\n",
    "\n",
    "Since $e^{i\\pi} = -1$, this preserves sign through exponentiation.\n",
    "\n",
    "**Bottom line:** The CSSM achieves stable and parallel computation of recurrent convolutions without exploding or vanishing gradients, and which can serve as the foundation for more complex recurrent architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HQi3MGjO8Xo"
   },
   "source": [
    "## 2. The Transformer's Trick: Instant Attention\n",
    "\n",
    "Transformers revolutionized deep learning with **self-attention**:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) V$$\n",
    "\n",
    "### What This Means\n",
    "\n",
    "Every position in the input can instantly \"attend to\" every other position:\n",
    "- **Q (Query)**: \"What am I looking for?\"\n",
    "- **K (Key)**: \"What do I contain?\"\n",
    "- **V (Value)**: \"What information do I carry?\"\n",
    "\n",
    "The $QK^T$ product computes similarity between all pairs of positions **in one shot**.\n",
    "\n",
    "### The Good\n",
    "\n",
    "- **Fully parallelizable**: No sequential dependencies\n",
    "- **Global context**: Any position can see any other position\n",
    "- **Scales well**: GPUs love matrix multiplications\n",
    "\n",
    "### The Catch\n",
    "\n",
    "- **O(N²) complexity**: Comparing all pairs of N positions is expensive\n",
    "- **All-or-nothing**: Attention is computed once, no refinement in the given layer. Depth is needed, for example, to develop more advanced attentional operations (e.g., perceptual groups)\n",
    "- **No iteration**: Can't \"change your mind\" or backtrack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTu_gPZrO8Xo"
   },
   "source": [
    "## 3. When Iterative Attention Helps: The Pathfinder Task\n",
    "\n",
    "Some visual tasks require **iterative reasoning**—you can't solve them in one glance.\n",
    "\n",
    "### The Pathfinder Challenge\n",
    "\n",
    "The task: **Are the two dots connected by a continuous curve?**\n",
    "\n",
    "The image contains:\n",
    "- Two marker dots (endpoints)\n",
    "- A potential connecting contour (curved path)\n",
    "- Distractor curves (noise to confuse you)\n",
    "\n",
    "**Why is this hard?**\n",
    "- It's not really that hard for Humans! But it can take time to solve (although usually still $<$ 1 second)\n",
    "- The contour can be long and winding\n",
    "- Distractors look similar to the real path\n",
    "- You need to \"trace\" the curve step by step\n",
    "\n",
    "### Why Iterative Attention?\n",
    "\n",
    "Imagine tracing a contour with your finger:\n",
    "\n",
    "1. **Start at one dot**\n",
    "2. **Follow the curve locally** (look at nearby pixels)\n",
    "3. **Keep extending** your search\n",
    "4. **Backtrack if stuck** (hit a dead end? try another direction)\n",
    "5. **Succeed when you reach the other dot**\n",
    "\n",
    "This is fundamentally **iterative**—you make local decisions that accumulate into a global answer.\n",
    "\n",
    "### The Transformer Problem\n",
    "\n",
    "A standard transformer computes attention once and makes a decision. But for Pathfinder:\n",
    "- **Early layers** can only see local structure\n",
    "- **To trace a long curve**, you need many layers stacked (deep = slow)\n",
    "- **No backtracking**: if an early layer makes a mistake, later layers can't fix it\n",
    "\n",
    "### The RNN Solution (and its flaw)\n",
    "\n",
    "An RNN can iterate naturally:\n",
    "- Each timestep refines the answer\n",
    "- Information spreads gradually across the image\n",
    "- More timesteps = larger effective receptive field\n",
    "- The growing receptive field is smart: it can ignore irrelevant stuff, it can become sensitive to curvy lines, etc.\n",
    "\n",
    "**So why not just use an RNN?**: Traditional RNNs are sequential → O(T) time → slow on GPUs. This is a bad fit for the architecture of state-of-the-art GPUs, which are designed for massive parallelization and minimal data transfer across the chip. RNNs are sequential and require intense data transfer, especially in the case of vision RNNs, since large activities for intermediate timesteps have to be shuttled in and out of fast memory. See the following for an expanded discussion: Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv:2312.00752"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Load and visualize actual Pathfinder-14 images\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# Load sample images (works both locally and on Colab)\nif IN_COLAB:\n    import urllib.request\n    SAMPLE_URL = \"https://connectomics.clps.brown.edu/tf_records/pathfinder_samples.npz\"\n    if not os.path.exists(\"pathfinder_samples.npz\"):\n        print(\"Downloading Pathfinder samples...\")\n        urllib.request.urlretrieve(SAMPLE_URL, \"pathfinder_samples.npz\")\n    data = np.load(\"pathfinder_samples.npz\")\n    pos_img, neg_img = data['pos'], data['neg']\nelse:\n    # Load from TFRecords locally\n    import tensorflow as tf\n    tf.config.set_visible_devices([], 'GPU')\n    \n    TFRECORD_DIR = '/home/dlinsley/pathfinder_tfrecord/difficulty_14/val'\n    \n    def parse_example(example):\n        features = tf.io.parse_single_example(example, {\n            'image': tf.io.FixedLenFeature([], tf.string),\n            'label': tf.io.FixedLenFeature([], tf.int64),\n        })\n        image = tf.io.decode_raw(features['image'], tf.float32)\n        image = tf.reshape(image, [224, 224, 3])\n        return image, features['label']\n    \n    val_files = sorted(tf.io.gfile.glob(f'{TFRECORD_DIR}/*.tfrecord'))\n    ds = tf.data.TFRecordDataset(val_files[:1]).map(parse_example)\n    \n    pos_img, neg_img = None, None\n    for img, label in ds:\n        if label.numpy() == 1 and pos_img is None:\n            pos_img = img.numpy()\n        elif label.numpy() == 0 and neg_img is None:\n            neg_img = img.numpy()\n        if pos_img is not None and neg_img is not None:\n            break\n\n# Display the images\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\naxes[0].imshow(pos_img)\naxes[0].set_title('CONNECTED (Positive)\\nThe two dots ARE linked by a contour', fontsize=13)\naxes[0].axis('off')\n\naxes[1].imshow(neg_img)\naxes[1].set_title('DISCONNECTED (Negative)\\nThe two dots are NOT linked', fontsize=13)\naxes[1].axis('off')\n\nplt.suptitle('Pathfinder-14: Can you trace the connecting contour?', fontsize=15, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTask: Determine if the two bright dots are connected by a continuous curve.\")\nprint(\"Challenge: Distractor curves make it hard—you must trace carefully!\")",
   "metadata": {
    "id": "sJj-WvZ3O8Xo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dh_xvgp_O8Xo"
   },
   "source": [
    "## 4. CSSM: A Parallel RNN (But Limited)\n",
    "\n",
    "CSSM offers a potential solution: **an RNN that runs in parallel**.\n",
    "\n",
    "### The Associative Scan Trick\n",
    "\n",
    "For linear recurrences of the form:\n",
    "\n",
    "$$h_{t+1} = A \\cdot h_t + b_t$$\n",
    "\n",
    "We can use the **associative scan** to compute all timesteps in O(log T) parallel time instead of O(T) sequential time.\n",
    "\n",
    "**How?** The operation $(A_2, b_2) \\circ (A_1, b_1) = (A_2 A_1, A_2 b_1 + b_2)$ is associative, so we can reorganize the computation into a tree:\n",
    "\n",
    "```\n",
    "t=0    t=1    t=2    t=3    t=4    t=5    t=6    t=7\n",
    "  \\    /        \\    /        \\    /        \\    /\n",
    "   [0:1]         [2:3]         [4:5]         [6:7]      ← Level 1\n",
    "      \\          /                \\          /\n",
    "       [0:3]                       [4:7]                ← Level 2\n",
    "           \\                      /\n",
    "                  [0:7]                                 ← Level 3\n",
    "```\n",
    "\n",
    "Instead of 8 sequential steps, we need only 3 parallel levels.\n",
    "\n",
    "### The Limitation\n",
    "\n",
    "Basic CSSM computes attention as a function of a **single state variable**:\n",
    "\n",
    "$$h_{t+1} = \\lambda \\cdot h_t + \\text{input}$$\n",
    "\n",
    "This is like a transformer with only **one** of Q, K, or V—severely limited expressiveness.\n",
    "\n",
    "**Can we make a CSSM that benefits from the Query-Key interaction of transformers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqm3NkiJO8Xo"
   },
   "source": [
    "## 5. Insights from the hGRU\n",
    "\n",
    "A key hint came from the **hGRU (horizontal Gated Recurrent Unit)**—an RNN we developed that successfully solves Pathfinder.\n",
    "\n",
    "### The hGRU's Secret: Bilinear Interactions\n",
    "\n",
    "The hGRU has two interacting cell populations:\n",
    "- **Excitatory cells (X)**: Spread activation along contours\n",
    "- **Inhibitory cells (Y)**: Suppress distractors\n",
    "\n",
    "The critical insight is the **bilinear term**:\n",
    "\n",
    "$$Y_{t+1} \\propto X_t \\odot Y_t$$\n",
    "\n",
    "**In plain English:** Inhibition is computed as the **product** of excitatory and inhibitory activity. This multiplicative interaction is much more expressive than simple addition.\n",
    "\n",
    "### Growing Receptive Fields = Growing Attention\n",
    "\n",
    "Each timestep, information spreads through a spatial kernel. After T timesteps:\n",
    "\n",
    "- **t=1**: Each pixel sees its immediate neighbors\n",
    "- **t=4**: Each pixel sees a moderate neighborhood  \n",
    "- **t=8**: Each pixel sees a large region\n",
    "\n",
    "The kernel **compounds over time**, creating an effective receptive field that grows with each iteration.\n",
    "\n",
    "**This is like attention with a growing radius!**\n",
    "\n",
    "At early timesteps, comparisons are local. At later timesteps, comparisons span the entire image. This allows the network to:\n",
    "1. **Start with local edge detection**\n",
    "2. **Gradually integrate** into longer contours\n",
    "3. **Make global decisions** only when enough context is gathered\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "The hGRU works great, but it's a traditional RNN—**sequential and slow**.\n",
    "\n",
    "**Can we preserve these bilinear, growing-receptive-field dynamics in a parallelizable CSSM?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SC2aX46O8Xp"
   },
   "source": [
    "---\n",
    "# Part II: TransformerCSSM\n",
    "---\n",
    "\n",
    "## 6. TransformerCSSM: Bringing It Together\n",
    "\n",
    "**TransformerCSSM** is our attempt to combine:\n",
    "- ✅ **Parallel computation** (from CSSM's associative scan)\n",
    "- ✅ **Query-Key interactions** (from transformers)\n",
    "- ✅ **Growing receptive fields** (from hGRU)\n",
    "- ✅ **Iterative refinement** (from RNNs)\n",
    "\n",
    "### The Three States\n",
    "\n",
    "We use transformer-inspired naming for three interacting state variables:\n",
    "\n",
    "| State | Name | Role | Intuition |\n",
    "|-------|------|------|-----------|\n",
    "| **Q** | Query | \"What am I looking for?\" | Current representation seeking context |\n",
    "| **K** | Key | \"What do I contain?\" | Information available to match against |\n",
    "| **A** | Attention | \"What have I found?\" | Accumulated Q-K correlations over time |\n",
    "\n",
    "### The Update Equations (ELI5 Version)\n",
    "\n",
    "**Query Update:**\n",
    "$$Q_{t+1} = \\underbrace{\\lambda_Q \\cdot Q_t}_{\\text{remember old Q}} + \\underbrace{w \\cdot (\\mathcal{K} * K_t)}_{\\text{look at K through kernel}} + \\underbrace{\\alpha \\cdot (\\mathcal{K} * A_t)}_{\\text{attention feedback}} + \\underbrace{U_Q}_{\\text{new input}}$$\n",
    "\n",
    "*\"The Query remembers itself, looks at what the Key contains (through a spatial kernel), gets modulated by accumulated Attention, and receives new input.\"*\n",
    "\n",
    "**Key Update:**\n",
    "$$K_{t+1} = \\underbrace{w \\cdot (\\mathcal{K} * Q_t)}_{\\text{look at Q through kernel}} + \\underbrace{\\lambda_K \\cdot K_t}_{\\text{remember old K}} + \\underbrace{U_K}_{\\text{new input}}$$\n",
    "\n",
    "*\"The Key looks at what the Query is seeking (symmetric to above!) and remembers itself.\"*\n",
    "\n",
    "**Attention Accumulator:**\n",
    "$$A_{t+1} = \\underbrace{\\gamma \\cdot (Q_t + K_t)}_{\\text{accumulate Q-K activity}} + \\underbrace{\\lambda_A \\cdot A_t}_{\\text{remember old A}} + \\underbrace{U_A}_{\\text{new input}}$$\n",
    "\n",
    "*\"Attention accumulates the sum of Q and K over time—building up a record of where Q and K agreed.\"*\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "1. **Q and K interact symmetrically** through a shared spatial kernel $\\mathcal{K}$\n",
    "2. **The kernel grows effective receptive field** over timesteps (like hGRU)\n",
    "3. **A accumulates Q-K correlation** over time (like building attention weights)\n",
    "4. **A feeds back into Q** (the accumulated attention modulates future queries)\n",
    "5. **Everything is linear** → can use associative scan → **parallel!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smkUU3NLO8Xp"
   },
   "outputs": [],
   "source": [
    "# Matrix form visualization\n",
    "print(\"=\"*70)\n",
    "print(\"TransformerCSSM as a 3x3 State Transition Matrix\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"  ┌                              ┐   ┌   ┐     ┌     ┐\")\n",
    "print(\"  │  λ_Q      w·K      α·K       │   │ Q │     │ U_Q │\")\n",
    "print(\"  │  w·K      λ_K       0        │ × │ K │  +  │ U_K │\")\n",
    "print(\"  │   γ        γ       λ_A       │   │ A │     │ U_A │\")\n",
    "print(\"  └                              ┘   └   ┘     └     ┘\")\n",
    "print()\n",
    "print(\"Where:\")\n",
    "print(\"  • λ_Q, λ_K, λ_A = decay rates (memory)\")\n",
    "print(\"  • w = Q↔K coupling weight (symmetric!)\")\n",
    "print(\"  • α = attention feedback strength (A → Q)\")\n",
    "print(\"  • γ = attention accumulation rate\")\n",
    "print(\"  • K = spatial convolution kernel (via FFT)\")\n",
    "print()\n",
    "print(\"Key insight: Q↔K coupling is SYMMETRIC (same w in both directions)\")\n",
    "print(\"This is like Q and K 'talking to each other' through the same channel\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ahATpX2O8Xp"
   },
   "source": [
    "### Sequential vs Parallel: The Best of Both Worlds\n",
    "\n",
    "**Sequential (traditional RNN):**\n",
    "```\n",
    "for t in range(T):\n",
    "    Q[t+1] = λ_Q·Q[t] + w·(K * K[t]) + α·(K * A[t]) + U\n",
    "    K[t+1] = w·(K * Q[t]) + λ_K·K[t] + U  \n",
    "    A[t+1] = γ·(Q[t] + K[t]) + λ_A·A[t] + U\n",
    "```\n",
    "Time: **O(T)** sequential steps\n",
    "\n",
    "**Parallel (associative scan):**\n",
    "```\n",
    "# Compute all timesteps simultaneously using tree reduction\n",
    "states = associative_scan(combine_fn, inputs)\n",
    "```\n",
    "Time: **O(log T)** parallel steps\n",
    "\n",
    "For T=8 timesteps: sequential needs 8 steps, parallel needs only 3!\n",
    "\n",
    "---\n",
    "# Part III: Hands-On Analysis\n",
    "---\n",
    "\n",
    "Now let's load a trained TransformerCSSM and see how it solves Pathfinder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xju6dl8SO8Xp"
   },
   "source": [
    "## 7. Loading the Model\n",
    "\n",
    "We'll load a TransformerCSSM trained on Pathfinder-14 (88.50% accuracy) and analyze:\n",
    "1. What the learned spatial kernel looks like\n",
    "2. How the model makes decisions over time\n",
    "3. Which mechanisms matter most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eJxXO9lO8Xp"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load checkpoint (uses CHECKPOINT_PATH from setup cell)\n",
    "with open(CHECKPOINT_PATH, 'rb') as f:\n",
    "    ckpt = pickle.load(f)\n",
    "\n",
    "params = ckpt['params']\n",
    "print(f\"Loaded checkpoint from epoch {ckpt.get('epoch', 'unknown')}\")\n",
    "print(f\"CSSM params: {list(params['cssm_0'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-67RRpPqO8Xp"
   },
   "outputs": [],
   "source": [
    "# Extract TransformerCSSM parameters\n",
    "cssm_params = params['cssm_0']\n",
    "\n",
    "# Single spatial kernel (NOT two like HGRUBi)\n",
    "K_kernel = jnp.array(cssm_params['kernel'])  # (C, k, k)\n",
    "print(f\"Spatial kernel shape: {K_kernel.shape}\")\n",
    "\n",
    "# Extract gate values - handle Dense layer dict structure\n",
    "def get_gate(name):\n",
    "    \"\"\"Extract gate value, handling Dense layer dict format.\"\"\"\n",
    "    gate_data = cssm_params[name]\n",
    "    # TransformerCSSM stores gates as Dense layers: {'kernel': ..., 'bias': ...}\n",
    "    if isinstance(gate_data, dict):\n",
    "        kernel = gate_data['kernel']\n",
    "        bias = gate_data.get('bias', 0)\n",
    "        val = kernel.mean() + (bias.mean() if hasattr(bias, 'mean') else bias)\n",
    "    else:\n",
    "        val = gate_data.mean()\n",
    "    return float(jax.nn.sigmoid(val))\n",
    "\n",
    "gates = {\n",
    "    'lambda_Q': get_gate('decay_Q'),\n",
    "    'lambda_K': get_gate('decay_K'),\n",
    "    'lambda_A': get_gate('decay_A'),\n",
    "    'w': get_gate('w_qk'),\n",
    "    'alpha': get_gate('alpha'),\n",
    "    'gamma': get_gate('gamma'),\n",
    "}\n",
    "\n",
    "print(\"\\nExtracted gate values:\")\n",
    "for name, val in gates.items():\n",
    "    print(f\"  {name:>10}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbrErI4tO8Xp"
   },
   "outputs": [],
   "source": [
    "# Visualize the single kernel (64 channels for KQV_64 model)\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "\n",
    "K_np = np.array(K_kernel)\n",
    "vmax = np.abs(K_np).max()\n",
    "\n",
    "for row in range(4):\n",
    "    for col in range(8):\n",
    "        idx = row * 8 + col\n",
    "        if idx < K_np.shape[0]:\n",
    "            axes[row, col].imshow(K_np[idx], cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "            axes[row, col].set_title(f'K[{idx}]', fontsize=8)\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('TransformerCSSM: Single Spatial Kernel (64 channels, shared for Q↔K and A→Q)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mean kernel\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "im = ax.imshow(K_np.mean(axis=0), cmap='RdBu_r')\n",
    "ax.set_title('Mean Kernel (averaged over 64 channels)', fontsize=12)\n",
    "ax.axis('off')\n",
    "plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYBHY_NaO8Xp"
   },
   "outputs": [],
   "source": [
    "# Load model for gradient computation\n",
    "from src.models.simple_cssm import SimpleCSSM\n",
    "\n",
    "model = SimpleCSSM(\n",
    "    num_classes=2,\n",
    "    embed_dim=64,  # 64-dim model (KQV_64)\n",
    "    depth=1,\n",
    "    cssm_type='transformer',  # TransformerCSSM!\n",
    "    kernel_size=15,\n",
    "    pos_embed='spatiotemporal',\n",
    "    seq_len=8,\n",
    ")\n",
    "print(\"TransformerCSSM model loaded (embed_dim=64).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8DsVOnMO8Xp"
   },
   "outputs": [],
   "source": "# Images already loaded earlier (pos_img, neg_img)\n# Let's display them again alongside the model's predictions\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\naxes[0].imshow(pos_img)\naxes[0].set_title('Positive Example (Connected)', fontsize=12)\naxes[0].axis('off')\naxes[1].imshow(neg_img)\naxes[1].set_title('Negative Example (Disconnected)', fontsize=12)\naxes[1].axis('off')\nplt.suptitle('Test Images for TransformerCSSM', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZmRz1ddO8Xp"
   },
   "outputs": [],
   "source": [
    "# Verify predictions\n",
    "def forward_single(img):\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "    logits = model.apply({'params': params}, x_temporal, training=False)\n",
    "    return logits[0]\n",
    "\n",
    "pos_logits = forward_single(pos_img)\n",
    "neg_logits = forward_single(neg_img)\n",
    "\n",
    "print(\"TransformerCSSM Predictions:\")\n",
    "print(f\"  Positive: {pos_logits} → {'Connected' if pos_logits.argmax() == 1 else 'Disconnected'}\")\n",
    "print(f\"  Negative: {neg_logits} → {'Connected' if neg_logits.argmax() == 1 else 'Disconnected'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL1uPhUAO8Xp"
   },
   "source": [
    "## 8. Temporal Gradient Attribution\n",
    "\n",
    "How does the model's decision depend on each timestep? We compute gradients of the output with respect to the input at each of the 8 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF1bn7k4O8Xq"
   },
   "outputs": [],
   "source": [
    "def compute_temporal_gradients(img, target_class):\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "\n",
    "    def forward_fn(x_t):\n",
    "        logits = model.apply({'params': params}, x_t, training=False)\n",
    "        return logits[0, target_class]\n",
    "\n",
    "    grads = jax.grad(forward_fn)(x_temporal)\n",
    "    grad_magnitude = jnp.abs(grads).sum(axis=(0, 2, 3, 4))\n",
    "    spatial_grads = jnp.abs(grads[0]).sum(axis=-1)\n",
    "    return grad_magnitude, spatial_grads\n",
    "\n",
    "pos_grad_mag, pos_spatial = compute_temporal_gradients(pos_img, 1)\n",
    "neg_grad_mag, neg_spatial = compute_temporal_gradients(neg_img, 0)\n",
    "\n",
    "print(\"Gradient magnitude per timestep:\")\n",
    "print(f\"  Positive: {np.array(pos_grad_mag).round(2)}\")\n",
    "print(f\"  Negative: {np.array(neg_grad_mag).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJJJrj3tO8Xq"
   },
   "outputs": [],
   "source": [
    "# Plot temporal importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "timesteps = np.arange(8)\n",
    "width = 0.35\n",
    "axes[0].bar(timesteps - width/2, np.array(pos_grad_mag), width,\n",
    "           label='Positive', color='green', alpha=0.7)\n",
    "axes[0].bar(timesteps + width/2, np.array(neg_grad_mag), width,\n",
    "           label='Negative', color='red', alpha=0.7)\n",
    "axes[0].set_xlabel('Timestep', fontsize=12)\n",
    "axes[0].set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "axes[0].set_title('TransformerCSSM: Temporal Importance', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(timesteps)\n",
    "\n",
    "# Cumulative\n",
    "pos_cumsum = np.cumsum(np.array(pos_grad_mag))\n",
    "neg_cumsum = np.cumsum(np.array(neg_grad_mag))\n",
    "axes[1].plot(timesteps, pos_cumsum / pos_cumsum[-1], 'g-o', label='Positive', linewidth=2)\n",
    "axes[1].plot(timesteps, neg_cumsum / neg_cumsum[-1], 'r-o', label='Negative', linewidth=2)\n",
    "axes[1].set_xlabel('Timestep', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Importance', fontsize=12)\n",
    "axes[1].set_title('Information Integration Over Time', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(timesteps)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCnEr7XnO8Xq"
   },
   "source": [
    "## 9. Mechanism Attribution: Which Components Matter?\n",
    "\n",
    "Which parts of the TransformerCSSM are most important for the decision? We compute gradients with respect to each learned parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CblgW8zMO8Xq"
   },
   "outputs": [],
   "source": [
    "def parameter_gradients(img, target_class):\n",
    "    x = jnp.array(img)[None, ...]\n",
    "    x_temporal = jnp.repeat(x[:, None, ...], 8, axis=1)\n",
    "\n",
    "    def loss_fn(p):\n",
    "        logits = model.apply({'params': p}, x_temporal, training=False)\n",
    "        return logits[0, target_class]\n",
    "\n",
    "    return jax.grad(loss_fn)(params)\n",
    "\n",
    "pos_param_grads = parameter_gradients(pos_img, 1)\n",
    "neg_param_grads = parameter_gradients(neg_img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "259aW9noO8Xq"
   },
   "outputs": [],
   "source": [
    "# TransformerCSSM-specific gates\n",
    "mechanisms = [\n",
    "    ('w_qk', 'w (Q↔K coupling)', 'Symmetric Q-K interaction'),\n",
    "    ('alpha', 'α (A→Q feedback)', 'Attention feeds back to Q'),\n",
    "    ('gamma', 'γ (Q,K→A accum)', 'Attention accumulation rate'),\n",
    "    ('decay_Q', 'λ_Q (Q memory)', 'Query state persistence'),\n",
    "    ('decay_K', 'λ_K (K memory)', 'Key state persistence'),\n",
    "    ('decay_A', 'λ_A (A memory)', 'Attention memory persistence'),\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TransformerCSSM MECHANISM ATTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Mechanism':<25} {'Description':<30} {'Pos':>10} {'Neg':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for key, name, desc in mechanisms:\n",
    "    if key in pos_param_grads['cssm_0']:\n",
    "        pos_mag = np.abs(np.array(pos_param_grads['cssm_0'][key])).mean()\n",
    "        neg_mag = np.abs(np.array(neg_param_grads['cssm_0'][key])).mean()\n",
    "        print(f\"{name:<25} {desc:<30} {pos_mag:>10.6f} {neg_mag:>10.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxIQeLUGO8Xq"
   },
   "outputs": [],
   "source": [
    "# Kernel gradient\n",
    "K_grad_pos = np.array(pos_param_grads['cssm_0']['kernel'])\n",
    "K_grad_neg = np.array(neg_param_grads['cssm_0']['kernel'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "im0 = axes[0].imshow(K_grad_pos.mean(axis=0), cmap='RdBu_r')\n",
    "axes[0].set_title('Kernel grad (Positive)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "plt.colorbar(im0, ax=axes[0], fraction=0.046)\n",
    "\n",
    "im1 = axes[1].imshow(K_grad_neg.mean(axis=0), cmap='RdBu_r')\n",
    "axes[1].set_title('Kernel grad (Negative)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "\n",
    "diff = K_grad_pos.mean(axis=0) - K_grad_neg.mean(axis=0)\n",
    "im2 = axes[2].imshow(diff, cmap='RdBu_r')\n",
    "axes[2].set_title('Difference (Pos - Neg)', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "\n",
    "plt.suptitle('Kernel Gradients: How Should K Change for Each Decision?', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmB3wnx5O8Xq"
   },
   "source": [
    "---\n",
    "# Part IV: Summary\n",
    "---\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "### The Problem\n",
    "- **Transformers** compute attention instantly but can't iterate or backtrack\n",
    "- **RNNs** can iterate but are sequential (slow on GPUs)\n",
    "- **Tasks like Pathfinder** require iterative, growing-receptive-field reasoning\n",
    "\n",
    "### The Solution: TransformerCSSM\n",
    "\n",
    "| Component | Inspiration | Benefit |\n",
    "|-----------|-------------|---------|\n",
    "| Q-K interaction | Transformers | Expressive attention-like computation |\n",
    "| Growing receptive field | hGRU | Local → global reasoning over time |\n",
    "| Associative scan | State Space Models | O(log T) parallel computation |\n",
    "| Attention accumulator (A) | Novel | Memory of Q-K correlations |\n",
    "\n",
    "### The Equations (Recap)\n",
    "\n",
    "$$Q_{t+1} = \\lambda_Q Q_t + w (\\mathcal{K} * K_t) + \\alpha (\\mathcal{K} * A_t) + U_Q$$\n",
    "$$K_{t+1} = w (\\mathcal{K} * Q_t) + \\lambda_K K_t + U_K$$\n",
    "$$A_{t+1} = \\gamma (Q_t + K_t) + \\lambda_A A_t + U_A$$\n",
    "\n",
    "### Results\n",
    "\n",
    "| Model | Architecture | Pathfinder-14 Accuracy |\n",
    "|-------|--------------|------------------------|\n",
    "| Standard Transformer | 12 layers, attention | ~75% |\n",
    "| hGRU (sequential RNN) | Bilinear E-I dynamics | ~90% |\n",
    "| **TransformerCSSM** | Q-K-A with parallel scan | **88.50%** |\n",
    "\n",
    "TransformerCSSM achieves near-hGRU performance while being **parallelizable**—the best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5N2qRwpdO8Xq"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"1. CSSM = RNN that runs in parallel via associative scan\")\n",
    "print()\n",
    "print(\"2. Basic CSSM is limited (single state variable)\")\n",
    "print()\n",
    "print(\"3. TransformerCSSM adds Q-K-A dynamics inspired by:\")\n",
    "print(\"   • Transformer attention (Q-K interaction)\")\n",
    "print(\"   • hGRU (bilinear terms, growing receptive fields)\")\n",
    "print()\n",
    "print(\"4. The model achieves 88.50% on Pathfinder-14\")\n",
    "print(\"   (comparable to sequential hGRU, but parallelizable!)\")\n",
    "print()\n",
    "print(\"5. Key mechanisms:\")\n",
    "print(\"   • Symmetric Q↔K coupling through spatial kernel\")\n",
    "print(\"   • A accumulates Q-K history (attention memory)\")\n",
    "print(\"   • A feeds back into Q (attention modulation)\")\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}